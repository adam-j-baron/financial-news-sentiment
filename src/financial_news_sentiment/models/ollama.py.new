"""Implementation of the Ollama-based sentiment analysis model."""
import asyncio
from typing import Any, List, Optional

from langchain_community.llms.ollama import Ollama

from ..exceptions import ModelError
from .base import BaseSentimentModel


class OllamaModel(BaseSentimentModel):
    """Ollama-based model for sentiment analysis."""

    def __init__(
        self,
        name: str = "qwen3",
        base_url: Optional[str] = None,
        system_prompt: Optional[str] = None,
        timeout: float = 30.0,
        **kwargs: Any,
    ) -> None:
        """Initialize the Ollama model.

        Args:
            name: Name of the model (e.g., 'qwen3')
            base_url: Base URL for the Ollama API (default: http://localhost:11434)
            system_prompt: System prompt for sentiment analysis
            timeout: API call timeout in seconds
            **kwargs: Additional configuration options
        """
        super().__init__(name, **kwargs)
        self.base_url = base_url or "http://localhost:11434"
        self.system_prompt = system_prompt or (
            "You are a financial sentiment analyzer. "
            "Read the text and determine if it expresses a positive (2), "
            "neutral (1), or negative (0) sentiment. "
            "Respond with ONLY the number representing the sentiment."
        )
        self.llm = Ollama(
            base_url=self.base_url,
            model=name,
            temperature=0.0,
            stop=["\n"],  # Stop at newlines to get clean responses
            timeout=timeout,
            num_ctx=512,  # Keep context window small for single predictions
        )

    async def initialize(self) -> None:
        """Initialize the connection to the Ollama service.

        Raises:
            ModelError: If there is an error connecting to Ollama
        """
        if self._is_initialized:
            return

        try:
            # Test the model by making a simple request
            await self.llm.ainvoke("Test connection.")
            self._is_initialized = True
        except Exception as e:
            raise ModelError(
                "Failed to initialize Ollama client",
                component="ollama",
                details=str(e)
            )

    async def predict(self, text: str) -> int:
        """Predict sentiment for a given text.

        Args:
            text: Input text to analyze

        Returns:
            int: Predicted sentiment label (0=negative, 1=neutral, 2=positive)

        Raises:
            ModelError: If there is an error during prediction
            ModelError: If the model has not been initialized
        """
        if not self._is_initialized:
            raise ModelError(
                "Model not initialized. Call initialize() first.",
                component="ollama"
            )

        try:
            prompt = f"{self.system_prompt}\n\nText: {text}\n\nSentiment:"
            response = await self.llm.ainvoke(prompt)
            cleaned = response.strip()

            try:
                sentiment = int(cleaned)
                if sentiment not in {0, 1, 2}:
                    raise ValueError(f"Invalid sentiment value: {sentiment}")
                return sentiment
            except (ValueError, TypeError) as e:
                raise ValueError(f"Could not parse sentiment from: {cleaned}")

        except Exception as e:
            raise ModelError(
                "Failed to predict sentiment",
                component="ollama",
                details=str(e)
            )

    async def predict_batch(self, texts: List[str]) -> List[int]:
        """Predict sentiment for multiple texts.

        Args:
            texts: List of texts to analyze

        Returns:
            list[int]: List of predicted sentiment labels
                      (0=negative, 1=neutral, 2=positive)

        Raises:
            ModelError: If prediction fails
            ModelError: If the model has not been initialized
        """
        if not self._is_initialized:
            raise ModelError(
                "Model not initialized. Call initialize() first.",
                component="ollama"
            )

        # Use a semaphore to limit concurrent API calls
        semaphore = asyncio.Semaphore(5)  # Max 5 concurrent requests

        async def predict_with_semaphore(text: str) -> int:
            async with semaphore:
                return await self.predict(text)

        try:
            # Predict all texts concurrently with rate limiting
            tasks = [predict_with_semaphore(text) for text in texts]
            return await asyncio.gather(*tasks)
        except Exception as e:
            raise ModelError(
                "Failed to generate batch predictions",
                component="ollama",
                details=str(e)
            )
