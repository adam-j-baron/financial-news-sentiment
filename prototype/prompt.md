Act as a senior software engineer specializing in Python production systems. Follow these instructions precisely to refactor the prototype into a robust, extensible application.  The financial-news-sentiment-prototype.ipynb is a proof-of-concept notebook prototype demonstrating how to load an evaluation dataset, evaluate models and make model classifications on live data.  Your task is to transform this prototype into production quality Python code.

Evaluation Datasets
 - The prototype loads just one evaluation dataset, Financial PhraseBank, but the production code should be able to handle multiple types of evaluation datasets.
 - Use a design that allows for plugging in different types of evaluation datasets from different datasets.
 - However, don't introduce new evaluation datasets and do not synthesize evaluation datasets (except in unit tests).
 - Only implement the Financial PhraseBank, as seen in the prototype code.
 - Ensure dataset splits are reproducible (via fixed random seeds).

 Models
 - The prototype code loads two types of models:
    - Transformer, in this case FinBERT
    - LLM, in this case Qwen3 via Ollama
 - Create a design so the production code has the flexibility to handle many other types of models.
 - However, don't introduce any new models and do not mock up a fake model (except in unit tests).
 - Only implement the FinBERT and Ollama Qwen3 models, as done in the prototype code.

Evaluation Metrics
- The prototype uses some fairly standard classification evaluation metrics.  The production code should be able to handle many other types of evaluation metrics.
- Only implement the sklearn evaluation metrics as seen in the prototype (e.g., accuracy_score and classification_report from scikit-learn).
- Make a design that allows for different evaluation metrics to be added.
- However, don't introduce any new evaluation metrics or mock up fake metrics (except in unit tests).
- Only implement the sklearn evaluation metrics as seen in the prototype.

Live Data
- The prototype enables access to live data, so the models can make classifications going forward.  The prototype uses news data from Yahoo! Finance.
- The intention is to allow humans to review the results and make their own judgement on how the models performed.
- Design a solution that is flexible enough to allow for other live datasets to be added.
- However, don't introduce any new live data feeds and do not synthesize live data (except in unit tests).
- Only implement the yfinance news live data as seen in the prototype.

Documentation
- Use descriptive code comments and functions descriptions.
- Create a README.md explaining how to run this production system. Ensure the README.md describes each module in great detail (e.g., purpose, key functions, extensibility points). Also, include a Markdown-compatible design diagram (using Mermaid syntax) embedded in the README.md to visualize the architecture (e.g., data flow from dataset loading to evaluation and live classification). Provide step-by-step installation instructions (e.g., 'pip install -r requirements.txt', 'python src/main.py --config config.yaml'), Docker setup if applicable, and how to run unit tests. Mention that this project was generated by an AI Agent using the provided prompt.md and prototype notebook in the /prototype folder.
    - A Quick Start example (run a sample evaluation, fetch live news, show sample output).
    - A section on Extending the system (how to add a new dataset, model, or metric using ABCs).
- Docstrings in Google or NumPy style for all public functions/classes.
- In docstring style, private/internal methods should be exempt (to avoid clutter).
- API reference generation via pdoc or sphinx (optional, but recommended).
- Include a make docs command that builds API documentation using pdoc or Sphinx, and ensure all public modules/classes are included.”
- Require at least minimal doc build config so users can type make docs or similar after install.
- A small contribution guide in docs/ (how to run tests, linting, black, etc.).
- For each plugin interface (DatasetLoader, Model, Evaluator), generate a module-level docstring specifying its calling contract with input/output examples.
    """
    BaseModel
    ---------
    Subclass and implement .predict(text: str) -> int | float.

    Example:
        class MyModel(BaseModel):
            def predict(self, text):
                ...
    """
- Enforce that all public classes and methods have a docstring. The CI should fail if docstring coverage is below threshold.

Extension Points Documentation
- Include UML-style class diagrams showing ABC inheritance patterns
- Provide concrete examples of how to add a new dataset loader
- Show step-by-step guide for adding a new model type
- Document the plugin architecture for metrics

Data Validation
- Implement input validation for all external data (news articles, dataset files)
- Add schema validation for API responses (yfinance, Ollama)
- Handle malformed or incomplete data gracefully with meaningful error messages
- Validate model outputs match expected format before processing

Configuration Enhancements
- Support configuration profiles (dev, staging, prod) with environment-specific overrides
- Add configuration validation that fails fast on startup with clear error messages
- Include configuration hot-reloading capability (optional, but useful for experimentation)
- If hot-reloading is not implemented, output a warning at startup indicating it as a TODO, to avoid silent fallback.
- Document all configuration options with examples and valid ranges

Logging
- Replace all print() calls with a structured logging framework (e.g., Python's logging module).
- The log level (e.g., INFO, DEBUG) and output handler (e.g., console, file) should be configurable via config.yaml. This is critical for debugging in a production environment without modifying code.
- Sanitize logs and all outputs to ensure no PII or sensitive content from processed live data is ever included in logs or error traces.
- Logs should be structured JSON in production (e.g., for ingestion into Elasticsearch or Datadog). Include: timestamp, log_level, module, function, message, and correlation_id (if applicable).

Monitoring & Observability
- Add metrics collection for model prediction times, accuracy rates, API response times
- Include health check endpoints if creating a web service
- Log prediction confidence scores and any model prediction failures
- Track cache hit/miss rates for performance optimization

Caching
- Implement a local caching mechanism for datasets and models to prevent re-downloading them on every run.
- Before fetching a resource, the code should check a local cache directory. If the resource exists, use the cached version.
- If caching is implemented, specify a cache invalidation strategy (e.g., clear cache if version mismatch or checksum differs).

Performance Requirements
- Add batch processing capabilities for evaluating multiple sentences simultaneously (especially for the FinBERT model to leverage GPU parallelization)
- Implement request rate limiting for Ollama API calls to prevent overwhelming the service
- Add progress bars/status indicators for long-running operations (dataset evaluation, live news processing)
- Consider memory management for large datasets (streaming/chunked processing if needed)

Security
- Implement input sanitization for all external text inputs (prevent injection attacks)
- Add request timeouts for all external API calls to prevent hanging
- Validate SSL certificates for HTTPS requests
- Implement proper secret management (never log sensitive config values)
- Secrets (e.g., API keys) must only be passed via environment variables or a vault service—never hardcoded or stored in config.yaml.

Testing Strategy
- Add property-based testing for model prediction consistency
- Include load testing for concurrent model evaluations
- Add regression tests to ensure model output consistency across versions
- Create smoke tests that can run quickly in CI for basic functionality validation
- Any synthetic/test/mock data should exist only in ‘tests/’ or ‘docs/’ and never be embedded in production modules or config defaults.
- Generate an end-to-end integration test that loads the PhraseBank, runs both models, evaluates, and prints a summary, with CLI options for test vs demo mode.

Deployment
- Include Docker Compose setup for local development with Ollama service
- Add health check script that validates all dependencies are working
- If the app runs as a service, include a /health endpoint that checks model availability, dataset accessibility, and API connectivity.
- Include deployment guide for common cloud platforms (AWS, GCP, Azure)
- Document resource requirements (CPU, memory, GPU if applicable)

Code Quality
- Add mypy for static type checking (not just optional)
- Include bandit for security linting
- Add pre-commit hooks for automatic code quality checks
- Require docstring coverage checking (e.g., interrogate)

Demonstration Features
- Include a comparison script that shows performance improvements over the prototype
- Add timing benchmarks comparing notebook vs production code execution
- Create a demo mode with sample data for quick showcasing
- Include before/after metrics in the README (execution time, memory usage, etc.)

CLI Features
- Add --dry-run flag for validation without execution
- Include --output-format option (json, csv, yaml) for results
- Add --parallel flag to control concurrent processing
- Include --profile flag for different configuration profiles

Async Implementation Details
- Refactor I/O-bound operations (e.g., calling the Ollama API, fetching news with yfinance, downloading datasets) to be asynchronous using Python's asyncio.
    - Dataset loading (I/O bound)
    - Model API calls (Ollama)
    - News article fetching (yfinance + newspaper)
    - Batch model predictions
- Use asyncio.Queue for managing concurrent model predictions
- Implement semaphores to limit concurrent Ollama API calls
- Use asyncio.gather with return_exceptions=True for batch operations
- Use async with for HTTP clients, database connections, and other resources to ensure proper cleanup even during exceptions.

Project Structure
- Organize the code into a standard Python project layout for clarity and maintainability. Use the following structure as a baseline:
    financial-news-sentiment/
    ├── .github/
    │   └── workflows/
    │       └── ci.yml
    ├── config/
    │   ├── config.yaml
    │   └── .env.example
    ├── docs/
    ├── prototype/
    │   ├── financial-news-sentiment-prototype.ipynb
    │   └── prompt.md
    ├── src/
    │   └── financial_sentiment/
    │       ├── __init__.py
    │       ├── datasets.py
    │       ├── evaluators.py
    │       ├── live_data.py
    │       ├── main.py
    │       └── models.py
    ├── tests/
    │   ├── test_datasets.py
    │   └── ...
    ├── .gitignore
    ├── README.md
    └── pyproject.toml
- Ensure the project is pip-installable (e.g., via setup.py or pyproject.toml) so users can install it as a package.
- Use Poetry for dependency management and packaging. Replace requirements.txt and setup.py with a single, well-defined pyproject.toml managed by Poetry. This ensures reproducible builds through a lock file.
- Dependency pinning in poetry.lock and  Dependabot for updates.

Implementation Phases
1. Core Infrastructure: Project structure, configuration, logging, basic CLI
2. Core Functionality: Dataset loading, model implementation, evaluation
3. Production Features: Async operations, caching, monitoring
4. Quality & Testing: Comprehensive test suite, CI/CD, documentation
5. Advanced Features: Performance optimization, deployment guides

Design Considerations
- For extensibility, use design patterns like abstract base classes (ABCs) or factory methods. For example, create a base DatasetLoader class with a load() method, and subclass it for FinancialPhraseBank. Similarly for models (base Model class with predict() method) and metrics (base Evaluator class).
- Use a configuration file (e.g., config.yaml) to manage parameters like dataset paths, model URLs/names, metric selections, or live data tickers. Load configs using libraries like PyYAML. This allows easy switching without code changes.
- For configuration, use a library like Pydantic to define a typed Settings model. This model should load parameters from config.yaml and environment variables, providing automatic type casting, validation (e.g., ensuring a URL is valid), and clear error messages for misconfigurations.
    - Require schema validation of config.yaml (so misconfigurations fail fast at startup).
    - Require documenting all config options in README.md (key, type, default, required/optional).
- Use Pydantic’s Field and validator to enforce constraints (e.g., num_sentences > 0). Include example error messages for invalid configs.
- Include an example config.yaml in the project root (or config/ folder) with default values based on the prototype (e.g., ollama_url, ollama_model, num_sentences, live_news_ticker). Document it in the README.md with explanations for each key.
- Use environment variables for sensitive or dynamic settings (e.g., OLLAMA_URL via os.getenv with fallback to config), loaded via libraries like dotenv if needed. Include .env.example in the project for reference.
- Make a "prototype" folder and copy this prompt.md and the unmodified financial-news-sentiment-prototype.ipynb into that folder.  Make note in the project level README.md that this code project was generated by an AI Agent using this prompt.md and the financial-news-sentiment-prototype.ipynb.
    - Do NOT include any code or artifacts from the prototype (‘/prototype’ folder) in the production package (src/...). Use it only as a reference for logic and tests. All production code must be clean, modular, and new.
    - Under no circumstances should any code from the prototype notebook be copied verbatim into the production codebase. All production code must be rewritten from scratch using modular, tested, and documented patterns.
- Add robust error handling. Define a custom exception hierarchy inheriting from a common BaseAppException. Create specific exceptions like DatasetError, ModelError, and LiveDataError to allow for granular error handling.
    - Logging exception traces at DEBUG level while surfacing user-friendly error messages at INFO/ERROR.
    - Exceptions in async tasks should be handled gracefully (e.g., with asyncio.gather(..., return_exceptions=True)).
    - Correlation IDs or request IDs (optional, but useful in async/multi-model evaluation workflows).
    - Explicit mention of rotating log files if file logging is enabled.
    - Exception Hierarchy Example
        - BaseAppException
        - DatasetError
            - DatasetDownloadError
            - DatasetValidationError
        - ModelError
            - ModelLoadError
            - ModelPredictionError
        - LiveDataError
            - NewsAPIError
            - ArticleParsingError
    - All exceptions surfaced to end-users must be a subclass of BaseAppException and include enough context (e.g., which component, config block, or resource caused the error).
- Use an async-native HTTP client like httpx for all API calls to improve performance and prevent blocking.
- Use type hints (from typing module) on all functions and classes for better code readability and IDE support.
- Ensure code style consistency: Use black for formatting and flake8 for linting; include instructions in README.md on how to run them.
- Make the system runnable via a command-line interface using a modern library like Typer. It uses type hints to create a self-documenting and easy-to-maintain CLI.
    - CLI should support --version and --help.
    - Provide example commands in README for common use cases (e.g., evaluate dataset, fetch live news, run models).
- Handle dependencies and suggest using virtual environments or Docker. Include a multi-stage Dockerfile that results in a minimal, secure production image by separating the build environment from the final runtime environment.
- For git, include a .gitignore that covers Python artifacts, env vars, and secrets (e.g., no API keys in code; use environment variables instead).
- Add unit tests using pytest. Cover key components like dataset loading, model predictions, metric calculations, and live data fetching. Aim for at least 80% code coverage (include a coverage report instruction in README.md, e.g., 'pytest --cov=src --cov-fail-under=80'). Use mocking (e.g., unittest.mock) for external dependencies like APIs or model calls in tests. Do not synthesize real data in production code, but mocking/synthesizing is allowed in tests.
    - Require unit + integration tests separation (e.g., tests/unit/, tests/integration/).
    - Explicitly ask for mocking external APIs (Ollama, yfinance) in unit tests, but allow real calls in integration tests (if config-enabled).
    - Require CI to fail if coverage < threshold (helps enforce quality).
- The GitHub Actions workflow in .github/workflows/ should perform a comprehensive check on every push/pull request, including:
	1. Running black --check to enforce code formatting.
	2. Running flake8 to check for linting errors.
	3. Running pytest --cov=src --cov-fail-under=80 to execute tests and check for coverage.
	4. (Optional) Running mypy for static type checking.
- Naming conventions for tests should be (test_*.py).
- Matrix testing (Python 3.10, 3.11, 3.12).
    - Lint/type check/test steps run in parallel jobs to shorten feedback loop.
    - (Optional) Publishing coverage reports to Codecov or similar.
- Cache dependencies in CI for speed.
- Optional: build Docker image in CI to confirm production readiness.
- In the main package's __init__.py file (e.g., src/financial_sentiment/__init__.py), explicitly import the key classes and functions that form the public API of the library. This makes the package easier to use for other developers.
- Secrets never hardcoded; must come only from env vars/config.
    - Require .env and config.yaml excluded from version control by default (but keep .env.example and default config).
    - Optional: require dependency scanning (e.g., pip-audit or Safety) in CI to catch vulnerabilities.
- Graceful shutdown handling for async operations (cleanup tasks, closing sessions).
- Include semantic versioning strategy
- Add changelog generation
    - Create a CHANGELOG.md pre-filled with initial version info and usage instructions.
    - Use semantic versioning (MAJOR.MINOR.PATCH). Document breaking changes in CHANGELOG.md. The public API (e.g., BaseModel, DatasetLoader) must remain backward-compatible within a MAJOR version.
- Document backward compatibility policy
- Test on Python 3.10, 3.11, 3.12
- Specify minimum version requirements for dependencies
- Concrete implementations must be registered in a plugin registry, not hardwired via ‘if’ statements or direct instantiation, except in the CLI entrypoint/main script.
    - Use a registry pattern (e.g., MODEL_REGISTRY = {}) and decorator-based registration for plugins. For example:
        @register_model(name="finbert")
        class FinBERTModel(BaseModel):
            ...
    This ensures discoverability without hardcoded if/else chains.
- Try to refactor of any function that exceeds 50 lines, to prevent monolithic code, unless justified.
- When generating output, produce a file tree listing first, followed by labeled code blocks for each file (e.g., ```python src/financial_sentiment/models.py```). If the agent cannot output every file due to context size, state which files are omitted and how to generate them next.
- If the agent cannot complete a step (e.g., output context or API limitation), emit a clear TODO and next action for the user.
- Add any other production-quality design aspects that may not have been mentioned thus far.

Output Expectations
- Generate the full project as a directory structure, ready to be cloned into git.
- Ensure the code is Python 3.10+ compatible and runs without errors on the specified components.
- Focus on clean, idiomatic Python code that transforms the prototype's linear notebook flow into modular, reusable scripts.
- Avoid over-engineering: Stick to the prototype's functionality while applying production best practices.
- If generating the full project isn't feasible in this response format, describe the directory structure in detail and provide the complete code/content for each file (e.g., as code blocks). Ensure the output is copy-paste ready for creating the project manually.

Final Deliverable Requirements
- Complete, runnable codebase with all specified features
- Comprehensive README with before/after performance comparisons
- Working Docker setup with sample commands
- CI/CD pipeline that passes all checks
- Demo script showing prototype vs production differences
- Architecture documentation with Mermaid diagrams